# Scalable-Machine-Learning-on-Big-Data-using-Apache-Spark

**NB: This repository is created for Learning purposes**

This repository will cover the follwowing :
- gain a practical understanding of Apache Spark, and apply it to solve machine learning problems involving both small and big data
- understand how parallel code is written, capable of running on thousands of CPUs. 
- make use of large scale compute clusters to apply machine learning algorithms on Petabytes of data using Apache SparkML Pipelines. 
- eliminate out-of-memory errors generated by traditional machine learning frameworks when data doesn’t fit in a computer's main memory
- test thousands of different ML models in parallel to find the best performing one – a technique used by many successful Kagglers
- run SQL statements on very large data sets using Apache SparkSQL and the Apache Spark DataFrame API.

Prerequisites:
- basic Python programming
- basic Machine Learning 
- basic SQL skills

If the reader is interested in Learning all of this, please purshase the course at the following link: 
https://www.coursera.org/learn/machine-learning-big-data-apache-spark?
